---
title: "An Analysis of Food Security in USA - 2021"
author: "Team - 02: Akshay Verma, Aveline Mariya Shaji and Uugangerel Bold"
date: "`r Sys.Date()`"
output:
  html_document:
    css: bootstrap.css
    code_folding: hide
    number_sections: no
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
editor_options: 
  markdown: 
    wrap: 72
---

***

# Precap

This paper is an extension to the Exploratory Data Analysis of the **Analysis of Food Security in USA - 2021**. Fr5om the previous analysis, we brought down to 7 variables such as Family_Size, Household_Income, SNAP, Ethnicity, Citizenship_status, Hours_on_Jobs, Education_Level which has an effect on the FoodSecurity_score variable. The extended part will deal different classification techniques/models to predict the food insecurity.

***

#### The models/techniques that we have used here are:

 -- Logistic Regression
 
 -- KNN Algorithm
 
 -- Decision Tree

***

# Our SMART questions are below: 

--	Specific: To extend the EDA on the Census data to find out what combinations of socio-economic factors lead to food insecurity. 

--	Measurable: Measure the risk of food insecurity at different combinations of selected factors.

--	Achievable: Make a prediction model using logistic regression for the food insecurity. Extend the modelling using KNN and Random Forest.

--	Relevant: Food being the basic requirement of any human, this study can shed light on what the authorities and we ourselves can do in order to eradicate food insecurity.

--	Time-Oriented: Data set for the month of December 2021 is considered for the study and the study is expected to come-up with interesting results by early December 2022.

***

```{r, echo=FALSE, results='hide', message=FALSE}
library(dplyr)
library(ezids)
library(ggplot2)
library(epiR)
library(pROC)
library(smotefamily)
library(ROSE)
library(ggthemes)
library(caret)
library(car)
library(ROSE)

```

***

# Dataset

We have obtained the data from the US Census website using CPS data. The CSV file contains approximately a hundred thousand observations. 

The link to our dataset is: 
https://www2.census.gov/programs-surveys/cps/datasets/2021/supp/dec21pub.csv

***

```{r, echo=FALSE, cache=TRUE }
Food_Sec <- data.frame(read.csv("dec21pub.csv"))
```

```{r, echo=FALSE, results='hide', message=FALSE}


FS_Subset <- subset(Food_Sec, HRINTSTA == 001 & HRSUPINT == 001 & HRFS12MD != -9)
FS_Subset <- subset(FS_Subset, select = c(	"GESTFIPS",	"HRNUMHOU",	"HEFAMINC",	"HESP1",	"PTDTRACE",	"PRCITSHP",	"PEMJNUM",	"PEHRUSL1",	"PEEDUCA", "PRNMCHLD" , "HRFS12MD"))

  

FS_Subset <- FS_Subset %>% rename("States" = "GESTFIPS", "Family_Size" = "HRNUMHOU",	"Household_Income" = "HEFAMINC",	"SNAP" = "HESP1",	"Ethnicity" =	"PTDTRACE", "Citizenship_status" = "PRCITSHP",	"Number_of_Jobs" = "PEMJNUM",	"Hours_on_Jobs" = "PEHRUSL1" , "Education_Level" = "PEEDUCA" , "Number_of_children" = "PRNMCHLD",  "FoodSecurity_score" = "HRFS12MD")
```

# Features in Previous Dataset.

Converting all the columns to factors as they are all ordinal and looking at the data features

```{r, echo=FALSE, results='hide', message=FALSE}
## Converting the all the columns to factors as they are all ordinal(except the Id, but since it's categorical i'm converting it into a factor too)


FS_Subset[] <- lapply( FS_Subset, factor)

str(FS_Subset)

```

# Data Pre-processing

```{r, echo=FALSE, results='hide', message=FALSE}
levels(FS_Subset$'FoodSecurity_score') <- c( "High Food Security", "Marginal Food Security", "Low Food Security", "Very Low Food Security")


FS_Subset$FS_Status <- FS_Subset$FoodSecurity_score

levels(FS_Subset$FS_Status) <- c( "Food Secure", "Food Secure", "Food Insecure", "Food Insecure")

levels(FS_Subset$'Ethnicity') <- c('White only', 'Black only', 'American Indian, Alaskan native only', 'Asian Only', 'Hawaiian', 'White-black', 'White-AI', 'White-Asian', 'White-HP', 'Black-AI', 'Black-Asian', 'Black-HP', 'AI-Asian', 'AI-HP', 'Asian-HP', 'W-B-AI', 'W-B-A', 'W-B-HP', 'W-AI-A', 'W-AI-HP', 'W-A-HP', 'B-AI-A', 'W-B-AL-A', 'W-AI-A-HP', 'Other 3 race combo', 'Other 4 and 5 race combo')

levels(FS_Subset$'Citizenship_status') <- c('NATIVE, BORN IN THE UNITED STATES', 'NATIVE, BORN IN PUERTO RICO OR OTHER U.S. ISLAND AREAS', 'NATIVE, BORN ABROAD OF AMERICAN PARENT OR PARENTS', 'FOREIGN BORN, U.S. CITIZEN BY NATURALIZATION', 'FOREIGN BORN, NOT A CITIZEN OF THE UNITED STATES')
summary(FS_Subset$'Citizenship_status', title = "PRCITSHP")


```

Now we clean the data and make sure that, we select variables which are necessary for the models are chosen.
```{r, echo=FALSE, results='hide', message=FALSE}
table(FS_Subset$Ethnicity)

## Lets drop all levels that have less than 10 observations.

for(i in levels(FS_Subset$Ethnicity)) {
  if(count(subset(FS_Subset, Ethnicity == i)) < 10){
    print(i)
    FS_Subset <- FS_Subset[!(FS_Subset$Ethnicity %in% c(i)), ]
  }}

FS_Subset$Ethnicity <- droplevels(FS_Subset$Ethnicity)
     
table(FS_Subset$Ethnicity)
```



# LOGISTIC REGRESSION

A sort of generalized linear model (GLM) called logistic regression is used in statistical analysis to forecast a binary event (i.e., dependent variable) based on predetermined variables. It estimates the likelihood that an event will occur by fitting data to a logit function.

In this model, the binary outcome is whether the unit of observation (individuals within housing units) is Food Secure or not. The variables in the model will try to predict the binary outcome. Also, the data set used in the model is unbalanced. Hence we use weights in Logistic Regression. 

In this model, we also use weights 
```{r, results ='markup'}
set.seed(1)

sample <- sample(c(TRUE, FALSE), nrow(FS_Subset), replace = TRUE, prob = c(0.6, 0.4))

train <- FS_Subset[sample, ]
test  <- FS_Subset[!sample, ]

str(test)
str(train)
```

We now started by adding all the variables to logistic regression to see what happens.We later take out some predictors and add others or add interaction terms.

```{r, echo=FALSE, results='hide', message=FALSE}
logistic <- glm(FS_Status ~ Ethnicity +  Family_Size + Household_Income:SNAP + Citizenship_status + Number_of_Jobs + Education_Level   , data = train, family = "binomial")
```

## Results of Logistics Regression Model

The model is predicting on the testing data and finding the mean squared error.

```{r, results='markup'}

logistic_model1.prob <- predict(logistic, test, type = "response")
logistic_model1.pred = rep("Food Secure", dim(test)[1])
logistic_model1.pred[logistic_model1.prob > .5] = "Food Insecure"


tb <- table(logistic_model1.pred, test$FS_Status)
tb[1:2,2:1]


precision <- round(precision(tb[1:2,2:1])*100,2)
recall <- round(recall(tb[1:2,2:1])*100,1)


lg_model_graph <- data.frame(precision , recall )
p <- barplot(as.matrix(lg_model_graph),beside=TRUE, col=rgb(0.2,0.3,0.6,0.6), space = c(0.1, 0.1))
text(x = p, y = lg_model_graph -  2, labels = lg_model_graph)


```


After intense research and discussion, we decided that recall is important to us as we want to get our false negative lower as possible. False positives are inclusion errors, while false negatives are exclusion errors. It's better if we are able to more accurately identify who all are food insecure and need of aid rather than identifying who all are not food insecure and hence don't need aid.

```{r, results='markup'}
train$logistic_model1_train.prob <- predict(logistic, train, type = "response")

# distribution of the prediction score grouped by known outcome
ggplot( train, aes( logistic_model1_train.prob, color = as.factor(FS_Status) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "Food Secure", "Food Insecure" ) ) + 
theme_economist()

```

```{r, results='markup'}
test$logistic_model1.prob <- logistic_model1.prob
roc_model1 <- roc(FS_Status ~ logistic_model1.prob, data = test)
plot(roc_model1)
auc(roc_model1)


```


Now we change the cutoff


```{r, results='markup'}
logistic_model_ncf.prob <- predict(logistic, test, type = "response")
logistic_model_ncf.pred = rep("Food Secure", dim(test)[1])
logistic_model_ncf.pred[logistic_model1.prob > .09] = "Food Insecure"


tb <- table(logistic_model_ncf.pred, test$FS_Status)
tb[1:2,2:1]


precision <- round(precision(tb[1:2,2:1])*100,2)
recall <- round(recall(tb[1:2,2:1])*100,1)


lg_model_graph <- data.frame(precision , recall )
p <- barplot(as.matrix(lg_model_graph),beside=TRUE, col=rgb(0.2,0.3,0.6,0.6), space = c(0.1, 0.1))
text(x = p, y = lg_model_graph -  2, labels = lg_model_graph)
```

```{r results='markup'}
sum(test$FS_Status == "Food Secure")/sum(test$FS_Status == "Food Insecure")

```

* So, our model has a 90% classification rate but the ratio of Food secure to Insecure in our data is 9.2. so this is a terrible model that's as good as tossing a coin. We will have to do something about the imbalance in our response and will have to clean our data further. Since all our variables are categorical of some kind, I highly recommend reading about categorical data modelling to figure out what to do.

* I think doing one-hot encoding(dummy variable) will help in dealing with the level issues in our predictors. We should also combine similar levels so we can reduce the number of levels. also the negative values which correspond to not in the universe or similar have to be dealt with in someway. we can either choose to omit those observation or try to find a proxy for them. I will prefer the latter.

* For the response variable imbalance we can create a new samples where the distribution between the responses is equal. Maybe something like k-fold Cross-Validation would work but I am not sure and would have to read more about it. 



#### I am going to be adding weights to the data so our minority class, which is food insecurity, affects the model more.


```{r, echo=FALSE, results='hide', message=FALSE, cache = TRUE}
weight_minority_class = sum(FS_Subset$FS_Status == "Food Secure")/sum(FS_Subset$FS_Status == "Food Insecure")
for(i in seq_len(NROW(FS_Subset))){
  
if(FS_Subset$FS_Status[i] == "Food Insecure"){
  FS_Subset$Weight[i] = weight_minority_class

}
  else
    FS_Subset$Weight[i] = 1
}

FS_Subset$Weight <- as.numeric(FS_Subset$Weight)
summary(FS_Subset$Weight)

## Splitting again because train and test doesn't contain the weight column

set.seed(1)

sample <- sample(c(TRUE, FALSE), nrow(FS_Subset), replace = TRUE, prob = c(0.6, 0.4))

train <- FS_Subset[sample, ]
test  <- FS_Subset[!sample, ]

```



```{r, echo=FALSE, results='hide', message=FALSE}

logistic_weighted <- glm(FS_Status ~ Ethnicity +  Family_Size + Household_Income:SNAP + Citizenship_status + Number_of_Jobs + Education_Level   , data = train,  weights = Weight, family = "binomial")


```


```{r, results='markup'}

logistic_model2.prob <- predict(logistic_weighted, test, type = "response")
logistic_model2.pred = rep("Food Secure", dim(test)[1])
logistic_model2.pred[logistic_model2.prob > .5] = "Food Insecure"



tb <- table(logistic_model2.pred, test$FS_Status)
tb[1:2,2:1]


precision <- round(precision(tb[1:2,2:1])*100,2)
recall <- round(recall(tb[1:2,2:1])*100,1)


lg_model_graph <- data.frame(precision , recall )
p <- barplot(as.matrix(lg_model_graph),beside=TRUE, col=rgb(0.2,0.3,0.6,0.6), space = c(0.1, 0.1))
text(x = p, y = lg_model_graph -  2, labels = lg_model_graph)


```
* Our precision for this model is `r 2635/(2635+7474)`

* Our recall for this model is `r 2635/(2635+173)`

* As you can see even though our classification rate of the model went down and the precision of model went down drastically, we were able to raise our recall rate significantly. 

```{r, results='markup'}
test$logistic_model2.prob <- logistic_model2.prob
roc_model2 <- roc(FS_Status ~ logistic_model2.prob, data = test)
plot(roc_model2)
auc(roc_model2)



```


```{r, results='markup'}

train$logistic_model2_train.prob <- predict(logistic_weighted, train, type = "response")

# distribution of the prediction score grouped by known outcome
ggplot( train, aes( logistic_model2_train.prob, color = as.factor(FS_Status) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "Food Secure", "Food Insecure" ) ) + 
theme_economist()


```

***

* I think this is enough for logistic regression. We can try changing and adding more predictors, but largely i think this is enough. I wasn't abe to add a lot of important predictors like ethnicity as the model was giving some errors. We should try to figure out how to solve it.

* We are going to try to do random forests and KNN now.

* Working on the data is still our number 1 priority, if we can figure out factor selection and how to use the factors we can't use rn, that would be more helpful than just getting the model to work.

*** 

* Trying to balance data by undersampling 

```{r,echo=FALSE, results='hide', message=FALSE}

data.balanced.ou <- ovun.sample(FS_Status~., data=train, p=0.5,  seed=1, method="under")$data




```

```{r, echo=FALSE, results='hide', message=FALSE}
logistic_both <- glm(FS_Status ~ Ethnicity + Family_Size + Household_Income:SNAP + Citizenship_status + Number_of_Jobs + Education_Level   , data = data.balanced.ou,   family = "binomial")

```

```{r, results='markup'}

logistic_model3.prob <- predict(logistic_both, test, type = "response")
logistic_model3.pred = rep("Food Secure", dim(test)[1])
logistic_model3.pred[logistic_model3.prob > .5] = "Food Insecure"




tb <- table(logistic_model3.pred, test$FS_Status)
tb[1:2,2:1]


precision <- round(precision(tb[1:2,2:1])*100,2)
recall <- round(recall(tb[1:2,2:1])*100,1)


lg_model_graph <- data.frame(precision , recall )
p <- barplot(as.matrix(lg_model_graph),beside=TRUE, col=rgb(0.2,0.3,0.6,0.6), space = c(0.1, 0.1))
text(x = p, y = lg_model_graph -  2, labels = lg_model_graph)

```

```{r, results='markup'}
train$logistic_model3_train.prob <- predict(logistic_both, train, type = "response")
ggplot( train, aes( logistic_model3_train.prob, color = as.factor(FS_Status) ) ) + 
geom_density( size = 1 ) +
ggtitle( "Training Set's Predicted Score" ) + 
scale_color_economist( name = "data", labels = c( "Food Secure", "Food Insecure" ) ) + 
theme_economist()
```

## K-Nearest Neighbors

K-Nearest Neighbor is a Supervised ML Algorithm used to solve both Classification and Regression models. K-NN algorithm assumes the similarity between the new case /data and available cases and put the new case into the category that is most similar to the available categories.It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset. KNN models shows high accuracy for classification problems.

# Preparing Data for KNN

The preparation of Data for KNN starts with selecting variables. As KNN only accepts numeric variables in or it needs to be transformed to numeric. Since only ordinal variables can be transformed to numeric, we eliminate nominal variables and hence, the list of variables used for KNN model are - Family Size, Household Income, SNAP, Number of jobs, Hours of Job, Number of Children. The predicting variable FS_Status remains binary but numeric.
 
```{r, results='markup', echo=FALSE}
knn_data <- FS_Subset
knn_data$Family_Size <- as.numeric(FS_Subset$Family_Size)
knn_data$Household_Income <- as.numeric(FS_Subset$Household_Income)
knn_data$SNAP <- as.numeric(FS_Subset$SNAP)
knn_data$Number_of_Jobs <- as.numeric(FS_Subset$Number_of_Jobs)
knn_data$Hours_on_Jobs <- as.numeric(FS_Subset$Hours_on_Jobs)
knn_data$Number_of_children <- as.numeric(FS_Subset$Number_of_children)
knn_data$FS_Status <- as.numeric(FS_Subset$FS_Status)
num_data <- select_if(knn_data, is.numeric)
num_data <- as.data.frame(num_data[-c(8)])
num_data$FS_Status <- replace(num_data$FS_Status, num_data$FS_Status == 1, 3)
num_data$FS_Status <- replace(num_data$FS_Status, num_data$FS_Status == 2, 1)
num_data$FS_Status <- replace(num_data$FS_Status, num_data$FS_Status == 3, 2)
num_data$FS_Status <- replace(num_data$FS_Status, num_data$FS_Status == 2, 0)
str(num_data)
```

# Balanced Data for KNN

The data is unbalanced. The number of food secure units of observations are very few when compared to food secured. This issue is resolved in Logstic Regression by adding weights as a hypervariable. But for KNN, sampling methods needs to be used to resolve this isse. The two options are Under-sampling and Up-sampling. Even though, new methods are coming out like creating artificial data points, the accuracy of the model through such model is still debatable. 

This does not hide the fact that under-sampling can lead to data lose as we are discarding a lot of observation units and up-sampling will cause over-fitting in the model. Trying both techniques and finding the best model is a considerable solution but it depends on the resources and time. 

For the KNN model discussed below, we choose for Under-sampling. The results based on the under-sampled data are discussed here. The number of observations that are Food secure and Food insecure is now 35635 and 35837 respectively. Now the data is balanced and we continue with data pre-processing. 

```{r, results='markup', echo=FALSE}
data_balanced_under <- as.data.frame(ovun.sample(FS_Status ~ ., data = num_data, method = "both", N = nrow(num_data), seed = 1)$data)
table(data_balanced_under$FS_Status)
```
```{r,results='markup', echo=FALSE}
num_data <- data_balanced_under
str(num_data)
```

For preparing the data for KNN, we selected numerical variables. Scaling the data is the next step. The algorithm should not be biased towards variables with higher magnitude. To overcome this problem, bring down all the variables to the same scale. Feature Scaling is inevitable for KNN. If not scaled, the feature with large value range will dominate while calculating distances. KNN which uses Euclidean distance needs scaling when the features has a broad range of values.

```{r, echo=FALSE, results='hide', message=FALSE}
scaledata <- as.data.frame(scale(num_data, center = TRUE, scale = TRUE))
str(scaledata)
```

We also need to create test and train data sets, we will do this slightly differently by using the sample function. The 2 says create 2 data sets essentially, replacement means we can reset the random sampling across each vector and the probability gives sample the weight of the splits, 2/3 for train, 1/3 for test. So here 67% of the sample is taken as training data and 33% as testing data.

```{r, echo=FALSE, results='hide', message=FALSE}
set.seed(1000)
knn_sample <- sample(2, nrow(scaledata), replace=TRUE, prob=c(0.67, 0.33))
```

We then just need to use the new variable to create the test/train outputs, selecting the first four rows as they are the numeric data in the iris data set and we want to predict Species. And then we need to create our Y variables or labels need to input into the KNN function.

```{r, results='markup', echo=FALSE}
# X variables
knn_training <- scaledata[knn_sample==1, 1:6]
knn_test <- scaledata[knn_sample==2, 1:6]
# Y variable
knn.trainLabels <- num_data[knn_sample==1, 7]
knn.testLabels <- num_data[knn_sample==2, 7]
```

```{r}
# Loading package
library(e1071)
library(caTools)
library(class)
loadPkg("gmodels")
loadPkg("gmodels")
loadPkg("FNN")
loadPkg("caret")
library(class)
```

# Building KNN Model


```{r, results='markup', echo=FALSE, eval=FALSE}
for (kval in 3:16) 
  {
  print( paste("k = ", kval) )
  knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)
  knn_crosst <- CrossTable(knn.testLabels, knn_pred, prop.chisq = FALSE)
  
  knn_crosst
  # 
  cm = confusionMatrix(knn_pred, reference = as.factor(knn.testLabels )) # from caret library
  # print.confusionMatrix(cm)
  # 
  cmaccu = cm$overall['Accuracy']
  #print( paste("Total Accuracy = ", cmaccu ) )
  # print("Other metrics : ")
  # print(cm$byClass)
  # 
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  # cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
  ResultDf = rbind(ResultDf, cmt)
  print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
  print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
  }
```

```{r, results='markup', echo=FALSE}
xkabledply(ResultDf, "Total Accuracy Summary")
```

```{r, results='markup', echo=FALSE}
ggplot(ResultDf,aes(x = k, y = Total.Accuracy)) +
  geom_line(color = "maroon", size = 1.5) +
  geom_point(size = 3) + 
  labs(title = "Accuracy vs k")
```

```{r, results='markup', echo=FALSE}
library(class)
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=6)
knn_crosst <- gmodels::CrossTable(x = knn.testLabels, y = knn_pred, prop.chisq = FALSE)

```

```{r, results='markup'}
xkabledply(data.frame(cm$byClass), title=paste("k = ",6))
```

Total Number of Observations = 23430.
True Negative = 11040 :- 47% of the observations were correctly predicted as NOT Food Insecure
True Positive = 8537 :- 36.45% are correctly identified as Food Insecure.
There were 3164 cases of False Negatives (FN). The FN’s poses a potential threat for the reason that model predicts 13.50% of the total observations as Food Secure, but was actually food insecure and the main focus to increase the accuracy of the model is to reduce FN’s.
There were 679 cases of False Positives (FP) meaning 679 cases were actually food secure in nature but got predicted as food insecure. 
The total accuracy of the model is 83.59 % ((TN+TP)/Total) which shows that the model is good. But still there is scope of improvement.

```{r, results='markup'}
# create an empty dataframe to store the results from confusion matrices
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
```

## Trees

```{r}
# Loading package
library("caret")
library("dplyr")
library("data.tree")
library("caTools")
library("rpart.plot")
library("RColorBrewer")
library("rattle")
library("ISLR")
library("tree") 
library("rpart")
```
#Using Decision Tree


```{r, echo = T, fig.dim=c(6,4)}
tree1 <- rpart(FS_Status ~ Ethnicity + Family_Size + Household_Income + SNAP + Citizenship_status + Number_of_Jobs + Education_Level, data=data.balanced.ou, method="class")
printcp(tree1) # display the results 
plotcp(tree1) # visualize cross-validation results 
summary(tree1) # detailed summary of splits
# plot tree 
plotcp(tree1)
#text(tree1, use.n=TRUE, all=TRUE, cex=.8)
rpart.plot(tree1)

predict_unseen <-predict(tree1, test, type = 'class')
table_mat <- table(predict_unseen, test$FS_Status)
table_mat[2:1,2:1]
```



```{r}
accuracy_tune <- function(tree1) {
    predict_unseen <- predict(tree1, test, type = 'class')
    table_mat <- table(test$FS_Status, predict_unseen)
    accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
    accuracy_Test
}
control <- rpart.control(minsplit = 4,
    minbucket = round(5 / 3),
    maxdepth = 3,
    cp = 0.0001)
tune_fit <- rpart(FS_Status ~ Ethnicity + Family_Size + Household_Income + SNAP + Citizenship_status + Number_of_Jobs + Education_Level , data = data.balanced.ou, method = 'class', control = control)
accuracy_tune(tune_fit)
```

```{r}
tree2 <- rpart(FS_Status ~ Ethnicity + Family_Size + Household_Income + SNAP + Citizenship_status + Number_of_Jobs + Education_Level, data=data.balanced.ou, method="class", control = control)
printcp(tree2) # display the results 
plotcp(tree2) # visualize cross-validation results 
summary(tree2) # detailed summary of splits
# plot tree 
plotcp(tree2)
#text(tree1, use.n=TRUE, all=TRUE, cex=.8)
rpart.plot(tree2)
```

```{r, results='markup'}
predict_unseen <-predict(tree2, test, type = 'class')
table_mat <- table(predict_unseen, test$FS_Status)
table_mat[2:1,2:1]
```

