* Hi, this is the rmd file for project 2. We will be doing logistic regression to certain the likelihood of food insecurity in permutations of social factors



```{r, echo=FALSE, cache=TRUE }
Food_Sec <- data.frame(read.csv("dec21pub.csv"))
```



```{r, echo=FALSE, results='hide', message=FALSE}
library(dplyr)
library(ezids)
library(ggplot2)
library(epiR)
```

* Lets start with the variables we selected for the first project. We should replace some variables I think, but we'll do it along the way. I am just copying code from the first project for now. so we can get a base to build upon.

```{r, results='markup', echo=FALSE}


FS_Subset <- subset(Food_Sec, HRINTSTA == 001 & HRSUPINT == 001 & HRFS12MD != -9)
FS_Subset <- subset(FS_Subset, select = c(	"GESTFIPS",	"HRNUMHOU",	"HEFAMINC",	"HESP1",	"PTDTRACE",	"PRCITSHP",	"PEMJNUM",	"PEHRUSL1",	"PEEDUCA", "PRNMCHLD" , "HRFS12MD"))

  

FS_Subset <- FS_Subset %>% rename("States" = "GESTFIPS", "Family_Size" = "HRNUMHOU",	"Household_Income" = "HEFAMINC",	"SNAP" = "HESP1",	"Ethnicity" =	"PTDTRACE", "Citizenship_status" = "PRCITSHP",	"Number_of_Jobs" = "PEMJNUM",	"Hours_on_Jobs" = "PEHRUSL1" , "Education_Level" = "PEEDUCA" , "Number_of_children" = "PRNMCHLD",  "FoodSecurity_score" = "HRFS12MD")



## Converting the all the columns to factors as they are all ordinal(except the Id, but since it's categorical i'm converting it into a factor too)


FS_Subset[] <- lapply( FS_Subset, factor)

str(FS_Subset)

```



* Since we will be doing logistic regression, we have to reduce our response to two level only. If we complete this project before the deadline we can look into multinomial regression aswell.




```{r, results='markup', echo=FALSE, results='hide'}
levels(FS_Subset$'FoodSecurity_score') <- c( "High Food Security", "Marginal Food Security", "Low Food Security", "Very Low Food Security")


FS_Subset$FS_Status <- FS_Subset$FoodSecurity_score

levels(FS_Subset$FS_Status) <- c( "Food Secure", "Food Secure", "Food Insecure", "Food Insecure")

levels(FS_Subset$'Ethnicity') <- c('White only', 'Black only', 'American Indian, Alaskan native only', 'Asian Only', 'Hawaiian', 'White-black', 'White-AI', 'White-Asian', 'White-HP', 'Black-AI', 'Black-Asian', 'Black-HP', 'AI-Asian', 'AI-HP', 'Asian-HP', 'W-B-AI', 'W-B-A', 'W-B-HP', 'W-AI-A', 'W-AI-HP', 'W-A-HP', 'B-AI-A', 'W-B-AL-A', 'W-AI-A-HP', 'Other 3 race combo', 'Other 4 and 5 race combo')






levels(FS_Subset$'Citizenship_status') <- c('NATIVE, BORN IN THE UNITED STATES', 'NATIVE, BORN IN PUERTO RICO OR OTHER U.S. ISLAND AREAS', 'NATIVE, BORN ABROAD OF AMERICAN PARENT OR PARENTS', 'FOREIGN BORN, U.S. CITIZEN BY NATURALIZATION', 'FOREIGN BORN, NOT A CITIZEN OF THE UNITED STATES')
summary(FS_Subset$'Citizenship_status', title = "PRCITSHP")







```
* Splitting the data

```{r, results ='markup'}
set.seed(1)

sample <- sample(c(TRUE, FALSE), nrow(FS_Subset), replace = TRUE, prob = c(0.6, 0.4))

train <- FS_Subset[sample, ]
test  <- FS_Subset[!sample, ]

str(test)
str(train)
```

* Just adding all the variables in the in the logistic regression to see what happens.We can then take out some predictors and add others or add intercation terms. This will aslo serve as sample code if you all are stuck somewhere.


```{r, results='markup'}
logistic <- glm(FS_Status ~ States + Family_Size + Household_Income + SNAP + Citizenship_status + Number_of_Jobs + Education_Level   , data = train, family = "binomial")

summary(logistic)


```

* predicting on the testing data and finding the mean squared error for this model.

```{r, results='markup'}

logistic_model1.prob <- predict(logistic, test, type = "response")
logistic_model1.pred = rep("Food Secure", dim(test)[1])
logistic_model1.pred[logistic_model1.prob > .5] = "Food Insecure"

mean(logistic_model1.pred == test$FS_Status)

table(logistic_model1.pred, test$FS_Status)
```

```{r results='markup'}
sum(test$FS_Status == "Food Secure")/sum(test$FS_Status == "Food Insecure")

```

* So, our model has a 90% classification rate but the ratio of Food secure to Insecure in our data is 9.04. so this is a terrible model that's as good as tossing a coin. We will have to do something about the imbalance in our response and will have to clean our data further. Since all our variables are categorical of some kind, I highly recommend reading about categorical data modelling to figure out what to do.

* I think doing one-hot encoding(dummy variable) will help in dealing with the level issues in our predictors. We should also combine similar levels so we can reduce the number of levels. also the negative values which correspond to not in the universe or similar have to be dealt with in someway. we can either choose to omit those observation or try to find a proxy for them. I will prefer the latter.

* For the response variable imbalance we can create a new samples where the distribution between the responses is equal. Maybe something like k-fold Cross-Validation would work but I am not sure and would have to read more about it. 


