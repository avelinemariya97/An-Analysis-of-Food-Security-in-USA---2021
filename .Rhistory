library(ROSE)
data_balanced_under <- as.data.frame(ovun.sample(FS_Status ~ ., data = num_data, method = "both", N = nrow(num_data), seed = 1)$data)
table(data_balanced_under$FS_Status)
str(data_balanced_under)
head(data_balanced_under)
tail(data_balanced_under)
num_data <- data_balanced_under
scaledata <- as.data.frame(scale(num_data, center = TRUE, scale = TRUE))
set.seed(1000)
knn_sample <- sample(2, nrow(scaledata), replace=TRUE, prob=c(0.67, 0.33))
knn_training <- scaledata[knn_sample==1, 1:5]
knn_test <- scaledata[knn_sample==2, 1:5]
knn.trainLabels <- num_data[knn_sample==1, 6]
knn.testLabels <- num_data[knn_sample==2, 6]
# Loading package
library(e1071)
library(caTools)
library(class)
loadPkg("gmodels")
loadPkg("gmodels")
loadPkg("FNN")
loadPkg("caret")
library(class)
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=3)
#knn_pred
knn_crosst <- gmodels::CrossTable(x = knn.testLabels, y = knn_pred, prop.chisq = FALSE)
knn_training <- scaledata[knn_sample==1, 1:6]
knn_test <- scaledata[knn_sample==2, 1:6]
knn.trainLabels <- num_data[knn_sample==1, 7]
knn.testLabels <- num_data[knn_sample==2, 7]
# Loading package
library(e1071)
library(caTools)
library(class)
loadPkg("gmodels")
loadPkg("gmodels")
loadPkg("FNN")
loadPkg("caret")
library(class)
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=3)
#knn_pred
knn_crosst <- gmodels::CrossTable(x = knn.testLabels, y = knn_pred, prop.chisq = FALSE)
# create an empty dataframe to store the results from confusion matrices
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
library(caret)
cm <- confusionMatrix(knn_pred, reference = as.factor(knn.testLabels) )
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
kval = 3
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
for (kval in 4:11) {
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)
knn_crosst <- CrossTable(knn.testLabels, knn_pred, prop.chisq = FALSE)
print( paste("k = ", kval) )
knn_crosst
#
cm = confusionMatrix(knn_pred, reference = as.factor(knn.testLabels )) # from caret library
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
xkabledply(ResultDf, "Total Accuracy Summary")
ggplot(ResultDf,aes(x = k, y = Total.Accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
labs(title = "accuracy vs k")
for (kval in 4:100) {
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)
knn_crosst <- CrossTable(knn.testLabels, knn_pred, prop.chisq = FALSE)
print( paste("k = ", kval) )
knn_crosst
#
cm = confusionMatrix(knn_pred, reference = as.factor(knn.testLabels )) # from caret library
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
for (kval in 4:100) {
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)
knn_crosst <- CrossTable(knn.testLabels, knn_pred, prop.chisq = FALSE)
#print( paste("k = ", kval) )
#knn_crosst
#
cm = confusionMatrix(knn_pred, reference = as.factor(knn.testLabels )) # from caret library
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
#print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
#print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
#print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
xkabledply(ResultDf, "Total Accuracy Summary")
ggplot(ResultDf,aes(x = k, y = Total.Accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
labs(title = "accuracy vs k")
for (kval in 4:25) {
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)
knn_crosst <- CrossTable(knn.testLabels, knn_pred, prop.chisq = FALSE)
#print( paste("k = ", kval) )
#knn_crosst
#
cm = confusionMatrix(knn_pred, reference = as.factor(knn.testLabels )) # from caret library
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
#print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
#print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
#print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
xkabledply(ResultDf, "Total Accuracy Summary")
ggplot(ResultDf,aes(x = k, y = Total.Accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
labs(title = "accuracy vs k")
for (kval in 4:11) {
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)
knn_crosst <- CrossTable(knn.testLabels, knn_pred, prop.chisq = FALSE)
#print( paste("k = ", kval) )
#knn_crosst
#
cm = confusionMatrix(knn_pred, reference = as.factor(knn.testLabels )) # from caret library
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
#print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
#print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
#print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
# Loading package
library("caret")
library("dplyr")
library("data.tree")
library("caTools")
library("rpart.plot")
library("RColorBrewer")
library("rattle")
library("ISLR")
library("tree")
library("rpart")
tree1 <- rpart(FS_Status ~ Ethnicity + Family_Size + Household_Income + SNAP + Citizenship_status + Number_of_Jobs + Education_Level, data=data.balanced.ou, method="class")
data.balanced.ou <- ovun.sample(FS_Status~., data=train, p=0.5,  seed=1, method="under")$data
tree1 <- rpart(FS_Status ~ Ethnicity + Family_Size + Household_Income + SNAP + Citizenship_status + Number_of_Jobs + Education_Level, data=data.balanced.ou, method="class")
printcp(tree1) # display the results
plotcp(tree1) # visualize cross-validation results
summary(tree1) # detailed summary of splits
# plot tree
plotcp(tree1)
#text(tree1, use.n=TRUE, all=TRUE, cex=.8)
rpart.plot(tree1)
accuracy_tune <- function(tree1) {
predict_unseen <- predict(tree1, test, type = 'class')
table_mat <- table(test$FS_Status, predict_unseen)
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
accuracy_Test
}
control <- rpart.control(minsplit = 3,
minbucket = round(5 / 3),
maxdepth = 3,
cp = 0)
tune_fit <- rpart(FS_Status ~ Ethnicity + Family_Size + Household_Income + SNAP + Citizenship_status + Number_of_Jobs + Education_Level , data = data.balanced.ou, method = 'class', control = control)
accuracy_tune(tune_fit)
tree2 <- rpart(FS_Status ~ Ethnicity + Family_Size + Household_Income + SNAP + Citizenship_status + Number_of_Jobs + Education_Level, data=data.balanced.ou, method="class", control = control)
printcp(tree2) # display the results
plotcp(tree2) # visualize cross-validation results
summary(tree2) # detailed summary of splits
# plot tree
plotcp(tree2)
#text(tree1, use.n=TRUE, all=TRUE, cex=.8)
rpart.plot(tree2)
tree <- makeLearner("classif.rpart")
getParamSet(tree)
treeParamSpace <- makeParamSet(
makeIntegerParam("minsplit", lower = 5, upper = 20),
makeIntegerParam("minbucket", lower = 3, upper = 10),
makeNumericParam("cp", lower = 0.01, upper = 0.1),
makeIntegerParam("maxdepth", lower = 3, upper = 10))
randSearch <- makeTuneControlRandom(maxit = 200)
cvForTuning <- makeResampleDesc("CV", iters = 5)
set.seed(314)
churn_folds <- vfold_cv(data.balanced.ou, v = 5)
Tree_model <- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>% set_engine('rpart') %>%  set_mode('classification')
churn_recipe <- recipe(FS_Status ~ ., data = data.balanced.ou) %>%
step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
step_normalize(all_numeric(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes())
tree_workflow <- workflow() %>% add_model(Tree_model) %>% add_recipe(churn_recipe)
ggplot(ResultDf,aes(x = k, y = Total.Accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
labs(title = "accuracy vs k")
xkabledply(ResultDf, "Total Accuracy Summary")
logistic <- glm(FS_Status ~ Ethnicity + Family_Size + Household_Income:SNAP + Citizenship_status + Number_of_Jobs + Education_Level   , data = train, family = "binomial")
logistic_model1.prob <- predict(logistic, test, type = "response")
logistic_model1.pred = rep("Food Secure", dim(test)[1])
logistic_model1.pred[logistic_model1.prob > .5] = "Food Insecure"
tb <- table(logistic_model1.pred, test$FS_Status)
tb
precision <- round(precision(tb[1:2,2:1])*100,2)
set.seed(314)
churn_folds <- vfold_cv(data.balanced.ou, v = 5)
Tree_model <- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>% set_engine('rpart') %>%  set_mode('classification')
churn_recipe <- recipe(FS_Status ~ ., data = data.balanced.ou) %>%
step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
step_normalize(all_numeric(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes())
tree_workflow <- workflow() %>% add_model(Tree_model) %>% add_recipe(churn_recipe)
tree_grid <- grid_regular(cost_complexity(),
tree_depth(),
min_n(),
levels = 2)
set.seed(314)
tree_tuning <- tree_workflow %>%
tune_grid(resamples = churn_folds,
grid = tree_grid)
tree_tuning %>% show_best('roc_auc')
best_tree <- tree_tuning %>%
select_best(metric = 'roc_auc')
final_tree_workflow <- tree_workflow %>%
finalize_workflow(best_tree)
tree_wf_fit <- final_tree_workflow %>%
fit(data = data.balanced.ou)
xkabledply(ResultDf, "Total Accuracy Summary")
for (kval in 4:11) {
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)
knn_crosst <- CrossTable(knn.testLabels, knn_pred, prop.chisq = FALSE)
#print( paste("k = ", kval) )
#knn_crosst
#
cm = confusionMatrix(knn_pred, reference = as.factor(knn.testLabels )) # from caret library
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
#print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
#print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
#print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
xkabledply(ResultDf, "Total Accuracy Summary")
ggplot(ResultDf,aes(x = k, y = Total.Accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
labs(title = "accuracy vs k")
for (kval in 4:11) {
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)
knn_crosst <- CrossTable(knn.testLabels, knn_pred, prop.chisq = FALSE)
#print( paste("k = ", kval) )
#knn_crosst
#
cm = confusionMatrix(knn_pred, reference = as.factor(knn.testLabels )) # from caret library
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
#print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
#print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
#print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
ggplot(ResultDf,aes(x = k, y = Total.Accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
labs(title = "accuracy vs k")
xkabledply(ResultDf, "Total Accuracy Summary")
ggplot(ResultDf,aes(x = k, y = Total.Accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
labs(title = "accuracy vs k")
for (kval in 4:11)
{
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)
knn_crosst <- CrossTable(knn.testLabels, knn_pred, prop.chisq = FALSE)
print( paste("k = ", kval) )
knn_crosst
#
cm = confusionMatrix(knn_pred, reference = as.factor(knn.testLabels )) # from caret library
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
#print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
xkabledply(ResultDf, "Total Accuracy Summary")
ggplot(ResultDf,aes(x = k, y = Total.Accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
labs(title = "accuracy vs k")
# create an empty dataframe to store the results from confusion matrices
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
library(caret)
cm <- confusionMatrix(knn_pred, reference = as.factor(knn.testLabels) )
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
kval = 3
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
for (kval in 4:11)
{
knn_pred <- knn(train = knn_training, test = knn_test, cl=knn.trainLabels, k=kval)
knn_crosst <- CrossTable(knn.testLabels, knn_pred, prop.chisq = FALSE)
print( paste("k = ", kval) )
knn_crosst
#
cm = confusionMatrix(knn_pred, reference = as.factor(knn.testLabels )) # from caret library
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
#print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
xkabledply(ResultDf, "Total Accuracy Summary")
ggplot(ResultDf,aes(x = k, y = Total.Accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
labs(title = "accuracy vs k")
tree1 <- rpart(FS_Status ~ Ethnicity + Family_Size + Household_Income + SNAP + Citizenship_status + Number_of_Jobs + Education_Level, data=data.balanced.ou, method="class")
printcp(tree1) # display the results
plotcp(tree1) # visualize cross-validation results
summary(tree1) # detailed summary of splits
# plot tree
plotcp(tree1)
#text(tree1, use.n=TRUE, all=TRUE, cex=.8)
rpart.plot(tree1)
accuracy_tune <- function(tree1) {
predict_unseen <- predict(tree1, test, type = 'class')
table_mat <- table(test$FS_Status, predict_unseen)
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
accuracy_Test
}
control <- rpart.control(minsplit = 3,
minbucket = round(5 / 3),
maxdepth = 3,
cp = 0)
tune_fit <- rpart(FS_Status ~ Ethnicity + Family_Size + Household_Income + SNAP + Citizenship_status + Number_of_Jobs + Education_Level , data = data.balanced.ou, method = 'class', control = control)
accuracy_tune(tune_fit)
tree2 <- rpart(FS_Status ~ Ethnicity + Family_Size + Household_Income + SNAP + Citizenship_status + Number_of_Jobs + Education_Level, data=data.balanced.ou, method="class", control = control)
printcp(tree2) # display the results
plotcp(tree2) # visualize cross-validation results
summary(tree2) # detailed summary of splits
# plot tree
plotcp(tree2)
#text(tree1, use.n=TRUE, all=TRUE, cex=.8)
rpart.plot(tree2)
tree <- makeLearner("classif.rpart")
getParamSet(tree)
treeParamSpace <- makeParamSet(
makeIntegerParam("minsplit", lower = 5, upper = 20),
makeIntegerParam("minbucket", lower = 3, upper = 10),
makeNumericParam("cp", lower = 0.01, upper = 0.1),
makeIntegerParam("maxdepth", lower = 3, upper = 10))
randSearch <- makeTuneControlRandom(maxit = 200)
cvForTuning <- makeResampleDesc("CV", iters = 5)
FS_Subset <- subset(Food_Sec, HRINTSTA == 001 & HRSUPINT == 001 & HRFS12MD != -9)
FS_Subset <- subset(FS_Subset, select = c(	"GESTFIPS",	"HRNUMHOU",	"HEFAMINC",	"HESP1",	"PTDTRACE",	"PRCITSHP",	"PEMJNUM",	"PEHRUSL1",	"PEEDUCA", "PRNMCHLD" , "HRFS12MD"))
FS_Subset <- FS_Subset %>% rename("States" = "GESTFIPS", "Family_Size" = "HRNUMHOU",	"Household_Income" = "HEFAMINC",	"SNAP" = "HESP1",	"Ethnicity" =	"PTDTRACE", "Citizenship_status" = "PRCITSHP",	"Number_of_Jobs" = "PEMJNUM",	"Hours_on_Jobs" = "PEHRUSL1" , "Education_Level" = "PEEDUCA" , "Number_of_children" = "PRNMCHLD",  "FoodSecurity_score" = "HRFS12MD")
## Converting the all the columns to factors as they are all ordinal(except the Id, but since it's categorical i'm converting it into a factor too)
FS_Subset[] <- lapply( FS_Subset, factor)
str(FS_Subset)
Food_Sec <- data.frame(read.csv("dec21pub.csv"))
table(FS_Subset$Ethnicity)
## Lets drop all levels that have less than 10 observations.
for(i in levels(FS_Subset$Ethnicity)) {
if(count(subset(FS_Subset, Ethnicity == i)) < 10){
print(i)
FS_Subset <- FS_Subset[!(FS_Subset$Ethnicity %in% c(i)), ]
}}
FS_Subset$Ethnicity <- droplevels(FS_Subset$Ethnicity)
table(FS_Subset$Ethnicity)
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(FS_Subset), replace = TRUE, prob = c(0.6, 0.4))
train <- FS_Subset[sample, ]
test  <- FS_Subset[!sample, ]
str(test)
str(train)
logistic <- glm(FS_Status ~ Ethnicity + Family_Size + Household_Income:SNAP + Citizenship_status + Number_of_Jobs + Education_Level   , data = train, family = "binomial")
library(dplyr)
library(ezids)
library(ggplot2)
library(epiR)
library(pROC)
library(smotefamily)
library(ROSE)
library(ggthemes)
library(caret)
library(mlr)
library(gbm)
library(tidyr)
library(vip)
library(tidyverse)
library(tidymodels)
FS_Subset <- subset(Food_Sec, HRINTSTA == 001 & HRSUPINT == 001 & HRFS12MD != -9)
FS_Subset <- subset(FS_Subset, select = c(	"GESTFIPS",	"HRNUMHOU",	"HEFAMINC",	"HESP1",	"PTDTRACE",	"PRCITSHP",	"PEMJNUM",	"PEHRUSL1",	"PEEDUCA", "PRNMCHLD" , "HRFS12MD"))
FS_Subset <- FS_Subset %>% rename("States" = "GESTFIPS", "Family_Size" = "HRNUMHOU",	"Household_Income" = "HEFAMINC",	"SNAP" = "HESP1",	"Ethnicity" =	"PTDTRACE", "Citizenship_status" = "PRCITSHP",	"Number_of_Jobs" = "PEMJNUM",	"Hours_on_Jobs" = "PEHRUSL1" , "Education_Level" = "PEEDUCA" , "Number_of_children" = "PRNMCHLD",  "FoodSecurity_score" = "HRFS12MD")
## Converting the all the columns to factors as they are all ordinal(except the Id, but since it's categorical i'm converting it into a factor too)
FS_Subset[] <- lapply( FS_Subset, factor)
str(FS_Subset)
levels(FS_Subset$'FoodSecurity_score') <- c( "High Food Security", "Marginal Food Security", "Low Food Security", "Very Low Food Security")
FS_Subset$FS_Status <- FS_Subset$FoodSecurity_score
levels(FS_Subset$FS_Status) <- c( "Food Secure", "Food Secure", "Food Insecure", "Food Insecure")
levels(FS_Subset$'Ethnicity') <- c('White only', 'Black only', 'American Indian, Alaskan native only', 'Asian Only', 'Hawaiian', 'White-black', 'White-AI', 'White-Asian', 'White-HP', 'Black-AI', 'Black-Asian', 'Black-HP', 'AI-Asian', 'AI-HP', 'Asian-HP', 'W-B-AI', 'W-B-A', 'W-B-HP', 'W-AI-A', 'W-AI-HP', 'W-A-HP', 'B-AI-A', 'W-B-AL-A', 'W-AI-A-HP', 'Other 3 race combo', 'Other 4 and 5 race combo')
levels(FS_Subset$'Citizenship_status') <- c('NATIVE, BORN IN THE UNITED STATES', 'NATIVE, BORN IN PUERTO RICO OR OTHER U.S. ISLAND AREAS', 'NATIVE, BORN ABROAD OF AMERICAN PARENT OR PARENTS', 'FOREIGN BORN, U.S. CITIZEN BY NATURALIZATION', 'FOREIGN BORN, NOT A CITIZEN OF THE UNITED STATES')
summary(FS_Subset$'Citizenship_status', title = "PRCITSHP")
table(FS_Subset$Ethnicity)
## Lets drop all levels that have less than 10 observations.
for(i in levels(FS_Subset$Ethnicity)) {
if(count(subset(FS_Subset, Ethnicity == i)) < 10){
print(i)
FS_Subset <- FS_Subset[!(FS_Subset$Ethnicity %in% c(i)), ]
}}
FS_Subset$Ethnicity <- droplevels(FS_Subset$Ethnicity)
table(FS_Subset$Ethnicity)
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(FS_Subset), replace = TRUE, prob = c(0.6, 0.4))
train <- FS_Subset[sample, ]
test  <- FS_Subset[!sample, ]
str(test)
str(train)
logistic <- glm(FS_Status ~ Ethnicity + Family_Size + Household_Income:SNAP + Citizenship_status + Number_of_Jobs + Education_Level   , data = train, family = "binomial")
logistic_model1.prob <- predict(logistic, test, type = "response")
logistic_model1.pred = rep("Food Secure", dim(test)[1])
logistic_model1.pred[logistic_model1.prob > .5] = "Food Insecure"
tb <- table(logistic_model1.pred, test$FS_Status)
tb
precision <- round(precision(tb[1:2,2:1])*100,2)
